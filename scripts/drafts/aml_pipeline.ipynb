{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPwON9I+xAr+9sN40jn1SAF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/usermar445/aml_final_project/blob/main/scripts/aml_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4fkBdx4AvPP",
        "outputId": "3c8245b6-0a18-42a7-f6c2-e65ba9c6b34c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tabpfn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyKZAjGDLj3G",
        "outputId": "eea17493-9cbb-4a58-8637-34a61e297a75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tabpfn\n",
            "  Downloading tabpfn-0.1.9-py3-none-any.whl (156 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.6/156.6 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from tabpfn) (1.23.5)\n",
            "Requirement already satisfied: pyyaml>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from tabpfn) (6.0.1)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from tabpfn) (2.31.0)\n",
            "Requirement already satisfied: scikit-learn>=0.24.2 in /usr/local/lib/python3.10/dist-packages (from tabpfn) (1.2.2)\n",
            "Requirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from tabpfn) (2.1.0+cu118)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->tabpfn) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->tabpfn) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->tabpfn) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->tabpfn) (2023.11.17)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.2->tabpfn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.2->tabpfn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.2->tabpfn) (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->tabpfn) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->tabpfn) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->tabpfn) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->tabpfn) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->tabpfn) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->tabpfn) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->tabpfn) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9.0->tabpfn) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.9.0->tabpfn) (1.3.0)\n",
            "Installing collected packages: tabpfn\n",
            "Successfully installed tabpfn-0.1.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huZb56oZCIWr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "from sklearn.datasets import make_circles, make_classification, make_moons\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "from sklearn.inspection import DecisionBoundaryDisplay\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "import lightgbm as lgb\n",
        "\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.impute import SimpleImputer\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from tabpfn import TabPFNClassifier\n"
      ],
      "metadata": {
        "id": "rAf84LNPRWck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Functions"
      ],
      "metadata": {
        "id": "cSo-NQUW16Mo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tree data functions**"
      ],
      "metadata": {
        "id": "YGaSEWMH3tcr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# re one-hot-encodes wilderness area and soiltype for tree data set\n",
        "\n",
        "# has to be executed on the original df\n",
        "def re_one_hot_encode_soiltype(data, drop=True):\n",
        "  soil = data.iloc[:, 14:54]\n",
        "  data['soil_type'] = soil.idxmax(1)\n",
        "  data['soil_type'] = data['soil_type'].replace(data['soil_type'].unique(), np.arange(1, data['soil_type'].nunique()+1))\n",
        "  if drop:\n",
        "    cols = data.iloc[:, 14:54].columns.to_list()\n",
        "    data = data.drop(columns=cols)\n",
        "  return data\n",
        "\n",
        "def re_one_hot_encode_wilderness(data, drop=True):\n",
        "  wilderness = data[['Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3', 'Wilderness_Area4']].copy()\n",
        "  data['wilderness_area'] = wilderness.idxmax(1)\n",
        "  data['wilderness_area'] = data['wilderness_area'].replace(data['wilderness_area'].unique(), np.arange(1, data['wilderness_area'].nunique()+1))\n",
        "  if drop:\n",
        "    data = data.drop(['Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3', 'Wilderness_Area4'], axis=1)\n",
        "  return data\n",
        "\n",
        "def re_one_hot_encode_categorical(data, drop=True):\n",
        "  return re_one_hot_encode_wilderness(re_one_hot_encode_soiltype(data, drop), drop)\n",
        "\n",
        "# helper method to split in features and labels\n",
        "def get_features_labels_tree(data, values=False):\n",
        "  X = data.drop(\"Cover_Type\", axis=1)\n",
        "  y = data.loc[:, 'Cover_Type']\n",
        "  if values:\n",
        "    return[X.values, y.values]\n",
        "  return [X, y]"
      ],
      "metadata": {
        "id": "T-5Rb6usVphI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Heloc functions**"
      ],
      "metadata": {
        "id": "uWSmtl-f3ynT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# helper function to determine indexes of rows containing only na values\n",
        "def get_indexes_nan_rows(data):\n",
        "  all_na = data.isna().all(axis=1)\n",
        "  all_na = all_na[all_na==True]\n",
        "  return all_na.index\n",
        "\n",
        "# returns percentage of missing values per column\n",
        "def get_missing_values_per_columns(data, columns, threshold):\n",
        "  missing = {}\n",
        "  problematic = []\n",
        "  for column in columns:\n",
        "    col = data.loc[:, column]\n",
        "    nans = np.isnan(col)\n",
        "    nans = nans[nans == True]\n",
        "    missing_values = len(nans)/len(col)\n",
        "    missing.update({column: missing_values})\n",
        "    if missing_values >= threshold:\n",
        "      problematic.append(column)\n",
        "  return [missing, problematic]\n",
        "\n",
        "#imputes missing values\n",
        "#because most of the features are very skewed, median seems more reasonable\n",
        "def impute_heloc(data):\n",
        "  imp_median = SimpleImputer(missing_values=np.nan, strategy='median')\n",
        "  columns = data.columns\n",
        "  indexes = data.index\n",
        "  imp_median.fit(data.values)\n",
        "  data = imp_median.transform(data.values)\n",
        "  data = pd.DataFrame(data, columns=columns)\n",
        "  data = data.set_index(indexes)\n",
        "  return data\n",
        "\n",
        "# wrapper function that prepares complete data set\n",
        "def prepare_data_heloc_train(data, drop_columns):\n",
        "  print(\"Inital dimensions:\", data.values.shape)\n",
        "  print(\"Recode labels\")\n",
        "  # replace labels with int values (because final submission needs to be 0, 1)\n",
        "  data['RiskPerformance'] = data['RiskPerformance'].replace(('Bad','Good'), (0,1))\n",
        "  # columns were determined in EDA\n",
        "  print(\"Drop columns\")\n",
        "  data = data.drop(drop_columns, axis=1)\n",
        "  print(\"New dimensions:\", data.values.shape)\n",
        "  print(\"Split in features and labels\")\n",
        "  # split to replace nas -> has to be done because complete NaS rows are excluded from prediction/training and will just be assigned fixed binomial probability classification\n",
        "  dfX = data.drop(\"RiskPerformance\", axis=1)\n",
        "  dfy = pd.DataFrame(data.loc[:, 'RiskPerformance'])\n",
        "  print(\"Replace nas\")\n",
        "  dfX[dfX<0] = np.nan\n",
        "  # get na rows index\n",
        "  na_indexes = get_indexes_nan_rows(dfX)\n",
        "  #drop na rows\n",
        "  dfX = dfX.drop(na_indexes)\n",
        "  dfy = dfy.drop(na_indexes)\n",
        "  print(\"Impute nas\")\n",
        "  dfX = impute_heloc(dfX)\n",
        "  return dfX.join(dfy)\n",
        "\n",
        "# helper method to split in features and labels\n",
        "def get_features_labels_heloc(data, values=False):\n",
        "  X = data.drop(\"RiskPerformance\", axis=1)\n",
        "  y = data.loc[:, 'RiskPerformance']\n",
        "  if values:\n",
        "    return[X.values, y.values]\n",
        "  return [X, y]"
      ],
      "metadata": {
        "id": "qcyui6Ex0Ykp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Higgs functions**"
      ],
      "metadata": {
        "id": "3KHWp2u75A-d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# cleans and prepares data\n",
        "def clean_higgs_data(data, drop_columns, train=True):\n",
        "  print(\"Inital dimensions:\", data.values.shape)\n",
        "  if train:\n",
        "    # replace labels with int values (because final submission needs to be 0, 1)\n",
        "    data['Label'] = data['Label'].replace(('b','s'), (0,1))\n",
        "  #drop columns\n",
        "  data = data.drop(drop_columns, axis=1)\n",
        "  print(\"New dimensions:\", data.values.shape)\n",
        "  return data\n",
        "\n",
        "# helper function to get features and labels split\n",
        "def get_features_labels_higgs(data, values=False):\n",
        "  X = data.drop(\"Label\", axis=1)\n",
        "  y = data.loc[:, 'Label']\n",
        "  if values:\n",
        "    return[X.values, y.values]\n",
        "  return [X, y]"
      ],
      "metadata": {
        "id": "4UDO6dSJ5CZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prepare data**"
      ],
      "metadata": {
        "id": "RWzwDtoDPx-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_datasets_ready(tree, heloc, drop_columns_heloc, higgs, drop_columns_higgs):\n",
        "  # two versions of tree data: one with one-hot-encoding one with categorical (lgb)\n",
        "  print(\"Preparing tree data...\")\n",
        "  print(\"Inital dimensions \", tree.values.shape)\n",
        "  print(\"New dimensions \", tree.values.shape)\n",
        "  print(\"Preparing tree data 2nd version...\")\n",
        "  print(\"Inital dimensions \", tree.values.shape)\n",
        "  lgb = re_one_hot_encode_categorical(tree)\n",
        "  print(\"New dimensions \", lgb.values.shape)\n",
        "  print(\"Preparing heloc data...\")\n",
        "  heloc_train = prepare_data_heloc_train(heloc, drop_columns_heloc)\n",
        "  print(\"Preparing higgs data...\")\n",
        "  higgs_train = clean_higgs_data(higgs, drop_columns_higgs)\n",
        "  return [tree, heloc_train, higgs_train, lgb]"
      ],
      "metadata": {
        "id": "-FteTzqyPzqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_training_ready_data(data):\n",
        "  # first keep data as dataframes to extract columns names (used for LightGBM)\n",
        "  dfXs = [get_features_labels_tree(data[0])[0], get_features_labels_heloc(data[1])[0], get_features_labels_higgs(data[2])[0], get_features_labels_tree(data[3])[0]]\n",
        "  dfys = [get_features_labels_tree(data[0])[1], get_features_labels_heloc(data[1])[1], get_features_labels_higgs(data[2])[1], get_features_labels_tree(data[3])[1]]\n",
        "  column_names = [dat.columns.to_list() for dat in dfXs]\n",
        "\n",
        "  #only get values\n",
        "  X = [get_features_labels_tree(data[0], values=True)[0], get_features_labels_heloc(data[1], values=True)[0], get_features_labels_higgs(data[2], values=True)[0], get_features_labels_tree(data[3], values=True)[0]]\n",
        "  y = [get_features_labels_tree(data[0], values=True)[1], get_features_labels_heloc(data[1], values=True)[1], get_features_labels_higgs(data[2], values=True)[1], get_features_labels_tree(data[3], values=True)[1]]\n",
        "\n",
        "  #split in train and validation set\n",
        "  X_train_tree, X_test_tree, y_train_tree, y_test_tree = train_test_split(X[0], y[0], test_size=0.2, random_state=42)\n",
        "  X_train_heloc, X_test_heloc, y_train_heloc, y_test_heloc = train_test_split(X[1], y[1], test_size=0.2, random_state=42)\n",
        "  X_train_higgs, X_test_higgs, y_train_higgs, y_test_higgs = train_test_split(X[2], y[2], test_size=0.2, random_state=42)\n",
        "  X_train_tree_lgb, X_test_tree_lgb, y_train_tree_lgb, y_test_tree_lgb = train_test_split(X[3], y[3], test_size=0.2, random_state=42)\n",
        "\n",
        "  # create lists\n",
        "  X_train = [X_train_tree, X_train_heloc, X_train_higgs, X_train_tree_lgb]\n",
        "  X_test = [X_test_tree, X_test_heloc, X_test_higgs, X_test_tree_lgb]\n",
        "  y_train = [y_train_tree, y_train_heloc, y_train_higgs, y_train_tree_lgb]\n",
        "  y_test = [y_test_tree, y_test_heloc, y_test_higgs, y_test_tree_lgb]\n",
        "\n",
        "  # extract some controlling numbers for better overivew\n",
        "  n_features = [x.shape[1] for x in X]\n",
        "  n_rows = [x.shape[0] for x in X]\n",
        "  print(\"number of features \", n_features)\n",
        "  print(\"number of rows \", n_rows)\n",
        "  labels = [np.unique(goal) for goal in y]\n",
        "  n_labels = [len(np.unique(goal)) for goal in y]\n",
        "  print(\"labels: \", labels)\n",
        "  print(\"number of labels \", n_labels)\n",
        "\n",
        "\n",
        "  return X_train, X_test, y_train, y_test, column_names"
      ],
      "metadata": {
        "id": "x10-_H4SQnmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Compare classifiers**"
      ],
      "metadata": {
        "id": "bYg3kkE18NVR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_standard_classifiers(classifiers, classifier_names, x_train, x_test, y_train, y_test, data_names):\n",
        "  scores = []\n",
        "  for name, clf in zip(classifier_names, classifiers):\n",
        "          print(\"Classifer \" + name)\n",
        "          results = {\"model\": name, \"parameters\": clf.get_params()}\n",
        "          score = []\n",
        "          for xtrain, xtest, ytrain, ytest, data_name in zip(x_train, x_test, y_train, y_test, data_names):\n",
        "            print(\"Train \", data_name)\n",
        "            clf = make_pipeline(StandardScaler(), clf)\n",
        "            clf.fit(xtrain, ytrain)\n",
        "            print(\"Test\")\n",
        "            acc = clf.score(xtest, ytest)\n",
        "            score.append(acc)\n",
        "            results.update({data_name: acc})\n",
        "            print(\"Done\")\n",
        "          results.update({\"overall score\": np.mean(score)})\n",
        "          scores.append(results)\n",
        "  return scores"
      ],
      "metadata": {
        "id": "qTNetV0e8QN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Light GBM**"
      ],
      "metadata": {
        "id": "w-HdD-omGvRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_scores_lightgbm(x_train, x_test, y_train, y_test, data_names, feature_names):\n",
        "  results = {\"model\": \"lgb\"}\n",
        "  scores = []\n",
        "  param = {'num_leaves': 31, 'objective': 'multiclass', 'num_class': 8}\n",
        "  results.update({\"params\": param})\n",
        "  num_round = 10\n",
        "  for xtrain, xtest, ytrain, ytest, data_name, features in zip(x_train, x_test, y_train, y_test, data_names, feature_names):\n",
        "    print(\"Train \", data_name)\n",
        "    if data_name=='tree_lgb':\n",
        "        train_data = lgb.Dataset(xtrain, label=ytrain, feature_name=features, categorical_feature=['wilderness_area', 'soil_type'], free_raw_data=False)\n",
        "    train_data = lgb.Dataset(xtrain, label=ytrain, feature_name=features, free_raw_data=False)\n",
        "    #train_data.save_binary('train.bin')\n",
        "    bst = lgb.train(param, train_data, num_round)\n",
        "    print(\"Test\")\n",
        "    ypred = bst.predict(xtest)\n",
        "    pred = pd.DataFrame(ypred)\n",
        "    predicted = pred.idxmax(axis=1)\n",
        "    acc = accuracy_score(ytest, predicted)\n",
        "    results.update({data_name: acc})\n",
        "    scores.append(acc)\n",
        "  results.update({\"overall score\": np.mean(scores)})\n",
        "  return results"
      ],
      "metadata": {
        "id": "s4zF5l_PFFuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TabPFN**"
      ],
      "metadata": {
        "id": "Sijq1qc8MEi3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_score_tabpfn(x_train, x_test, y_train, y_test, data_names):\n",
        "  results = {\"model\": \"TabPFN\"}\n",
        "  score = []\n",
        "  classifier = TabPFNClassifier(device='cpu', N_ensemble_configurations=32)\n",
        "  for xdata, xtest, ydata, ytest, data_name in zip(x_train,x_test, y_train, y_test, data_names):\n",
        "    sample_indexes =  np.random.randint(0, xdata.shape[0], 1024)\n",
        "    x_sample = xdata[sample_indexes, :]\n",
        "    y_sample = ydata[sample_indexes]\n",
        "    print(\"Train \", data_name)\n",
        "    classifier.fit(x_sample, y_sample)\n",
        "    print(\"Test\")\n",
        "    y_eval, p_eval = classifier.predict(xtest, return_winning_probability=True)\n",
        "    acc = accuracy_score(ytest, y_eval)\n",
        "    score.append(acc)\n",
        "    results.update({data_name: acc})\n",
        "    print(\"Done\")\n",
        "  results_tabpfn.update({\"overall score\": np.mean(score)})\n",
        "  return results"
      ],
      "metadata": {
        "id": "ElSZUgTRMDwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Make predictions**"
      ],
      "metadata": {
        "id": "4IwCVG-xYoGl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_prediction_heloc(train_set, drop_columns,  test_set, classifier, test_submission):\n",
        "  train = prepare_data_heloc_train(train_set, drop_columns)\n",
        "  test, na_indexes = prepare_data_heloc_test(test_set, drop_columns)\n",
        "  X_test = test.values\n",
        "  X,y = get_features_labels_heloc(train)\n",
        "  print(\"train\")\n",
        "  classifier = make_pipeline(StandardScaler(), classifier)\n",
        "  classifier.fit(X, y)\n",
        "  print(\"predict\")\n",
        "  y_pred = classifier.predict(X_test)\n",
        "  p = 5000/5459\n",
        "  na_preds = np.random.binomial(1, p, size=len(na_indexes))\n",
        "  y_pred[na_indexes] = na_preds\n",
        "  test_submission['pred'] = y_pred.astype(int)\n",
        "  test_submission = test_submission.drop('Prediction', axis=1)\n",
        "  test_submission = test_submission.rename(columns={'pred': 'Prediction'})\n",
        "  return test_submission"
      ],
      "metadata": {
        "id": "gepknaxTYmrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Load Train Data"
      ],
      "metadata": {
        "id": "eEaOusPL1kdi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tp_ZipWR1zac"
      },
      "outputs": [],
      "source": [
        "tree_train = pd.read_csv(\"/content/drive/MyDrive/aml/data/covtype_train.csv\")\n",
        "heloc_train = pd.read_csv(\"/content/drive/MyDrive/aml/data/heloc_train.csv\")\n",
        "higgs_train = pd.read_csv(\"/content/drive/MyDrive/aml/data/higgs_train.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "higgs_train.values.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkheZGWtY99L",
        "outputId": "c59690d3-d9a2-4f8c-e307-23792250f909"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(175000, 33)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Prepare data sets"
      ],
      "metadata": {
        "id": "O6tbnyf63N4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# final columns to drop\n",
        "# first 3 because of missing values\n",
        "# the rest because of multicorriliarity\n",
        "drop_columns_heloc = ['MSinceMostRecentDelq',\n",
        "                      'MSinceMostRecentInqexcl7days',\n",
        "                      'NetFractionInstallBurden',\n",
        "                      'NumTotalTrades',\n",
        "                      'MaxDelq2PublicRecLast12M',\n",
        "                      'MaxDelqEver',\n",
        "                      'NumInqLast6Mexcl7days',\n",
        "                      'NumTrades90Ever2DerogPubRec'\n",
        "                      ]"
      ],
      "metadata": {
        "id": "PUKqZqKQnJUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# final columns to drop\n",
        "# first 3 because of missing values\n",
        "# the rest because of multicorriliarity\n",
        "drop_columns_heloc2 = ['MSinceMostRecentDelq',\n",
        "                      'MSinceMostRecentInqexcl7days',\n",
        "                      'NetFractionInstallBurden'\n",
        "                      ]"
      ],
      "metadata": {
        "id": "DYbe5U9HTUm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# final list of columns to drop\n",
        "higgs_drop_columns = ['EventId',\n",
        "              'DER_deltaeta_jet_jet',\n",
        "              'DER_mass_jet_jet',\n",
        "              'DER_prodeta_jet_jet',\n",
        "              'DER_lep_eta_centrality',\n",
        "              'PRI_jet_subleading_pt',\n",
        "              'PRI_jet_subleading_eta',\n",
        "              'PRI_jet_subleading_phi',\n",
        "              'Weight',\n",
        "              'PRI_jet_num',\n",
        "              'PRI_met_phi',\n",
        "              'PRI_met_sumet',\n",
        "              'PRI_jet_leading_pt',\n",
        "              'PRI_jet_leading_eta',\n",
        "              'PRI_jet_leading_phi',\n",
        "              'PRI_jet_all_pt']"
      ],
      "metadata": {
        "id": "9Sbmm14V50yo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# final list of columns to drop\n",
        "higgs_drop_columns2 = ['EventId']"
      ],
      "metadata": {
        "id": "p1tUOh3RTYuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = make_datasets_ready(tree_train, heloc_train, drop_columns_heloc, higgs_train, higgs_drop_columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HZ2s1K9RajV",
        "outputId": "083b01ff-27e6-45a5-bd69-696eed317cb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing tree data...\n",
            "Inital dimensions  (58101, 56)\n",
            "New dimensions  (58101, 56)\n",
            "Preparing tree data 2nd version...\n",
            "Inital dimensions  (58101, 56)\n",
            "New dimensions  (58101, 13)\n",
            "Preparing heloc data...\n",
            "Inital dimensions: (9413, 24)\n",
            "Recode labels\n",
            "Drop columns\n",
            "New dimensions: (9413, 16)\n",
            "Split in features and labels\n",
            "Replace nas\n",
            "Impute nas\n",
            "Preparing higgs data...\n",
            "Inital dimensions: (175000, 33)\n",
            "New dimensions: (175000, 17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Make train and validation set"
      ],
      "metadata": {
        "id": "Hfizz6_p6Vbb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_names = ['tree',  'heloc', 'higgs', 'tree_lgb']"
      ],
      "metadata": {
        "id": "pnWKgIiCTAXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test, column_names = get_training_ready_data(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1Oj4n0DRrii",
        "outputId": "fae8cf6e-e413-4601-b1b7-98b94e47fb06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of features  [55, 15, 16, 12]\n",
            "number of rows  [58101, 8876, 175000, 58101]\n",
            "labels:  [array([1, 2, 3, 4, 5, 6, 7]), array([0, 1]), array([0, 1]), array([1, 2, 3, 4, 5, 6, 7])]\n",
            "number of labels  [7, 2, 2, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Compare classifiers"
      ],
      "metadata": {
        "id": "W6Gv4eUr7jAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifiers_names = [\n",
        "    \"Nearest Neighbors\",\n",
        "    \"Decision Tree\",\n",
        "    \"Random Forest\",\n",
        "    \"AdaBoost\",\n",
        "    \"Logistic Regression\"\n",
        "]\n",
        "\n",
        "classifiers = [\n",
        "    KNeighborsClassifier(3),\n",
        "    DecisionTreeClassifier(max_depth=10, random_state=42),\n",
        "    RandomForestClassifier(\n",
        "        max_depth=10, n_estimators=100, max_features=1, random_state=42\n",
        "    ),\n",
        "    AdaBoostClassifier(random_state=42),\n",
        "    LogisticRegression(random_state=42)\n",
        "]"
      ],
      "metadata": {
        "id": "KsHGPoRp7d7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = compare_standard_classifiers(classifiers, classifiers_names, X_train, X_test, y_train, y_test, data_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yb5QoNQW7e3w",
        "outputId": "ad880313-9d4d-4d73-ad8d-471607d1a3a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classifer Nearest Neighbors\n",
            "Train  tree\n",
            "Test\n",
            "Done\n",
            "Train  heloc\n",
            "Test\n",
            "Done\n",
            "Train  higgs\n",
            "Test\n",
            "Done\n",
            "Train  tree_lgb\n",
            "Test\n",
            "Done\n",
            "Classifer Decision Tree\n",
            "Train  tree\n",
            "Test\n",
            "Done\n",
            "Train  heloc\n",
            "Test\n",
            "Done\n",
            "Train  higgs\n",
            "Test\n",
            "Done\n",
            "Train  tree_lgb\n",
            "Test\n",
            "Done\n",
            "Classifer Random Forest\n",
            "Train  tree\n",
            "Test\n",
            "Done\n",
            "Train  heloc\n",
            "Test\n",
            "Done\n",
            "Train  higgs\n",
            "Test\n",
            "Done\n",
            "Train  tree_lgb\n",
            "Test\n",
            "Done\n",
            "Classifer AdaBoost\n",
            "Train  tree\n",
            "Test\n",
            "Done\n",
            "Train  heloc\n",
            "Test\n",
            "Done\n",
            "Train  higgs\n",
            "Test\n",
            "Done\n",
            "Train  tree_lgb\n",
            "Test\n",
            "Done\n",
            "Classifer Logistic Regression\n",
            "Train  tree\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test\n",
            "Done\n",
            "Train  heloc\n",
            "Test\n",
            "Done\n",
            "Train  higgs\n",
            "Test\n",
            "Done\n",
            "Train  tree_lgb\n",
            "Test\n",
            "Done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2B9ShnAODFqW",
        "outputId": "885253f6-6829-40dd-aa47-8593ccfcc95f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'model': 'Nearest Neighbors',\n",
              "  'parameters': {'algorithm': 'auto',\n",
              "   'leaf_size': 30,\n",
              "   'metric': 'minkowski',\n",
              "   'metric_params': None,\n",
              "   'n_jobs': None,\n",
              "   'n_neighbors': 3,\n",
              "   'p': 2,\n",
              "   'weights': 'uniform'},\n",
              "  'tree': 0.8389983650288271,\n",
              "  'heloc': 0.678490990990991,\n",
              "  'higgs': 0.7705714285714286,\n",
              "  'tree_lgb': 0.8311677136218915,\n",
              "  'overall score': 0.7798071245532845},\n",
              " {'model': 'Decision Tree',\n",
              "  'parameters': {'ccp_alpha': 0.0,\n",
              "   'class_weight': None,\n",
              "   'criterion': 'gini',\n",
              "   'max_depth': 10,\n",
              "   'max_features': None,\n",
              "   'max_leaf_nodes': None,\n",
              "   'min_impurity_decrease': 0.0,\n",
              "   'min_samples_leaf': 1,\n",
              "   'min_samples_split': 2,\n",
              "   'min_weight_fraction_leaf': 0.0,\n",
              "   'random_state': 42,\n",
              "   'splitter': 'best'},\n",
              "  'tree': 0.7657688667068239,\n",
              "  'heloc': 0.6762387387387387,\n",
              "  'higgs': 0.8182571428571429,\n",
              "  'tree_lgb': 0.7541519662679632,\n",
              "  'overall score': 0.7536041786426672},\n",
              " {'model': 'Random Forest',\n",
              "  'parameters': {'bootstrap': True,\n",
              "   'ccp_alpha': 0.0,\n",
              "   'class_weight': None,\n",
              "   'criterion': 'gini',\n",
              "   'max_depth': 10,\n",
              "   'max_features': 1,\n",
              "   'max_leaf_nodes': None,\n",
              "   'max_samples': None,\n",
              "   'min_impurity_decrease': 0.0,\n",
              "   'min_samples_leaf': 1,\n",
              "   'min_samples_split': 2,\n",
              "   'min_weight_fraction_leaf': 0.0,\n",
              "   'n_estimators': 100,\n",
              "   'n_jobs': None,\n",
              "   'oob_score': False,\n",
              "   'random_state': 42,\n",
              "   'verbose': 0,\n",
              "   'warm_start': False},\n",
              "  'tree': 0.7129334824885982,\n",
              "  'heloc': 0.7454954954954955,\n",
              "  'higgs': 0.8183714285714285,\n",
              "  'tree_lgb': 0.7364254367094054,\n",
              "  'overall score': 0.7533064608162319},\n",
              " {'model': 'AdaBoost',\n",
              "  'parameters': {'algorithm': 'SAMME.R',\n",
              "   'base_estimator': 'deprecated',\n",
              "   'estimator': None,\n",
              "   'learning_rate': 1.0,\n",
              "   'n_estimators': 50,\n",
              "   'random_state': 42},\n",
              "  'tree': 0.41399191119525,\n",
              "  'heloc': 0.7353603603603603,\n",
              "  'higgs': 0.8069142857142857,\n",
              "  'tree_lgb': 0.4943636520092935,\n",
              "  'overall score': 0.6126575523197973},\n",
              " {'model': 'Logistic Regression',\n",
              "  'parameters': {'C': 1.0,\n",
              "   'class_weight': None,\n",
              "   'dual': False,\n",
              "   'fit_intercept': True,\n",
              "   'intercept_scaling': 1,\n",
              "   'l1_ratio': None,\n",
              "   'max_iter': 100,\n",
              "   'multi_class': 'auto',\n",
              "   'n_jobs': None,\n",
              "   'penalty': 'l2',\n",
              "   'random_state': 42,\n",
              "   'solver': 'lbfgs',\n",
              "   'tol': 0.0001,\n",
              "   'verbose': 0,\n",
              "   'warm_start': False},\n",
              "  'tree': 0.7248945873849066,\n",
              "  'heloc': 0.7404279279279279,\n",
              "  'higgs': 0.7409428571428571,\n",
              "  'tree_lgb': 0.7065657000258153,\n",
              "  'overall score': 0.7282077681203768}]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_scores_lightgbm(X_train, X_test, y_train, y_test, data_names, column_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnXNGTJKDp8I",
        "outputId": "350d7179-1402-4da2-87b3-d627b90b44a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train  tree\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.092464 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2245\n",
            "[LightGBM] [Info] Number of data points in the train set: 46480, number of used features: 51\n",
            "[LightGBM] [Info] Start training from score -34.538776\n",
            "[LightGBM] [Info] Start training from score -0.997957\n",
            "[LightGBM] [Info] Start training from score -0.725996\n",
            "[LightGBM] [Info] Start training from score -2.778804\n",
            "[LightGBM] [Info] Start training from score -5.428657\n",
            "[LightGBM] [Info] Start training from score -4.121385\n",
            "[LightGBM] [Info] Start training from score -3.526404\n",
            "[LightGBM] [Info] Start training from score -3.341282\n",
            "Test\n",
            "Train  heloc\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000328 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1097\n",
            "[LightGBM] [Info] Number of data points in the train set: 7100, number of used features: 15\n",
            "[LightGBM] [Info] Start training from score -0.657182\n",
            "[LightGBM] [Info] Start training from score -0.730454\n",
            "[LightGBM] [Info] Start training from score -34.538776\n",
            "[LightGBM] [Info] Start training from score -34.538776\n",
            "[LightGBM] [Info] Start training from score -34.538776\n",
            "[LightGBM] [Info] Start training from score -34.538776\n",
            "[LightGBM] [Info] Start training from score -34.538776\n",
            "[LightGBM] [Info] Start training from score -34.538776\n",
            "Test\n",
            "Train  higgs\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022648 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4077\n",
            "[LightGBM] [Info] Number of data points in the train set: 140000, number of used features: 16\n",
            "[LightGBM] [Info] Start training from score -0.419191\n",
            "[LightGBM] [Info] Start training from score -1.071713\n",
            "[LightGBM] [Info] Start training from score -34.538776\n",
            "[LightGBM] [Info] Start training from score -34.538776\n",
            "[LightGBM] [Info] Start training from score -34.538776\n",
            "[LightGBM] [Info] Start training from score -34.538776\n",
            "[LightGBM] [Info] Start training from score -34.538776\n",
            "[LightGBM] [Info] Start training from score -34.538776\n",
            "Test\n",
            "Train  tree_lgb\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005571 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2170\n",
            "[LightGBM] [Info] Number of data points in the train set: 46480, number of used features: 12\n",
            "[LightGBM] [Info] Start training from score -34.538776\n",
            "[LightGBM] [Info] Start training from score -0.997957\n",
            "[LightGBM] [Info] Start training from score -0.725996\n",
            "[LightGBM] [Info] Start training from score -2.778804\n",
            "[LightGBM] [Info] Start training from score -5.428657\n",
            "[LightGBM] [Info] Start training from score -4.121385\n",
            "[LightGBM] [Info] Start training from score -3.526404\n",
            "[LightGBM] [Info] Start training from score -3.341282\n",
            "Test\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'model': 'lgb',\n",
              " 'params': {'num_leaves': 31, 'objective': 'multiclass', 'num_class': 8},\n",
              " 'tree': 0.7673177867653386,\n",
              " 'heloc': 0.7404279279279279,\n",
              " 'higgs': 0.8228,\n",
              " 'tree_lgb': 0.7642199466483091,\n",
              " 'overall score': 0.7736914153353938}"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_score_tabpfn(X_train, X_test, y_train, y_test, data_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "2YZymuBTHUnX",
        "outputId": "1d54bd11-3044-42eb-8a49-15415562078a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We have to download the TabPFN, as there is no checkpoint at  /usr/local/lib/python3.10/dist-packages/tabpfn/models_diff/prior_diff_real_checkpoint_n_0_epoch_100.cpkt\n",
            "It has about 100MB, so this might take a moment.\n",
            "Loading model that can be used for inference only\n",
            "Using a Transformer with 25.82 M parameters\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-541cb8f3ec9f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_score_tabpfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-25-ecac83d2af27>\u001b[0m in \u001b[0;36mget_score_tabpfn\u001b[0;34m(x_train, x_test, y_train, y_test, data_names)\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"TabPFN\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTabPFNClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gpu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_ensemble_configurations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mxdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mydata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0msample_indexes\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, device, base_path, model_string, N_ensemble_configurations, no_preprocess_mode, multiclass_decoder, feature_shift_decoder, only_inference, seed, no_grad, batch_size_inference)\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels_in_memory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m             model, c, results_file = load_model_workflow(i, -1, add_name=model_string, base_path=base_path, device=device,\n\u001b[0m\u001b[1;32m    151\u001b[0m                                                          eval_addition='', only_inference=only_inference)\n\u001b[1;32m    152\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels_in_memory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py\u001b[0m in \u001b[0;36mload_model_workflow\u001b[0;34m(i, e, add_name, base_path, device, eval_addition, only_inference)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0monly_inference\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loading model that can be used for inference only'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model_only_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;31m#until now also only capable of inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/model_builder.py\u001b[0m in \u001b[0;36mload_model_only_inference\u001b[0;34m(path, filename, device)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mmodel_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m         \"\"\"\n\u001b[1;32m   1140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1141\u001b[0;31m         \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_to_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected one of cpu, cuda, ipu, xpu, mkldnn, opengl, opencl, ideep, hip, ve, fpga, ort, xla, lazy, vulkan, mps, meta, hpu, mtia, privateuseone device type at start of device string: gpu"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Select model and tune"
      ],
      "metadata": {
        "id": "hhgbz_otYChP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gt2FuebIOLv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Make predictions"
      ],
      "metadata": {
        "id": "quPPqN-sYgTI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_prediction_heloc(train_set, drop_columns,  test_set, classifier, test_submission):\n",
        "  train = prepare_data_heloc_train(train_set, drop_columns)\n",
        "  test, na_indexes = prepare_data_heloc_test(test_set, drop_columns)\n",
        "  X_test = test.values\n",
        "  X,y = get_features_labels_heloc(train)\n",
        "  print(\"train\")\n",
        "  classifier = make_pipeline(StandardScaler(), classifier)\n",
        "  classifier.fit(X, y)\n",
        "  print(\"predict\")\n",
        "  y_pred = classifier.predict(X_test)\n",
        "  p = 5000/5459\n",
        "  na_preds = np.random.binomial(1, p, size=len(na_indexes))\n",
        "  y_pred[na_indexes] = na_preds\n",
        "  test_submission['pred'] = y_pred.astype(int)\n",
        "  test_submission = test_submission.drop('Prediction', axis=1)\n",
        "  test_submission = test_submission.rename(columns={'pred': 'Prediction'})\n",
        "  return test_submission"
      ],
      "metadata": {
        "id": "MrzKCw8T3x1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_heloc = pd.read_csv(\"/content/drive/MyDrive/aml/data/heloc_train.csv\")\n",
        "drop_columns_heloc = ['MSinceMostRecentDelq',\n",
        "                      'MSinceMostRecentInqexcl7days',\n",
        "                      'NetFractionInstallBurden',\n",
        "                      'NumTotalTrades',\n",
        "                      'MaxDelq2PublicRecLast12M',\n",
        "                      'MaxDelqEver',\n",
        "                      'NumInqLast6Mexcl7days',\n",
        "                      'NumTrades90Ever2DerogPubRec'\n",
        "                      ]\n",
        "heloc_test = pd.read_csv(\"/content/drive/MyDrive/aml/data/heloc_test.csv\")\n",
        "clf = RandomForestClassifier(max_depth=10, n_estimators=100, max_features=1, random_state=42)\n",
        "submission_heloc = pd.read_csv(\"/content/drive/MyDrive/aml/data/heloc_test_submission.csv\")\n",
        "\n",
        "\n",
        "submission = make_prediction_heloc(df_heloc, drop_columns_heloc, heloc_test, clf, submission_heloc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVHeBf3b5d4H",
        "outputId": "68a6c13d-1553-437d-8f91-7ed288fd6ba8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inital dimensions: (9413, 24)\n",
            "Recode labels\n",
            "Drop columns\n",
            "New dimensions: (9413, 16)\n",
            "Split in features and labels\n",
            "Replace nas\n",
            "Impute nas\n",
            "Inital dimensions: (1046, 23)\n",
            "Recode labels\n",
            "Drop columns\n",
            "New dimensions: (1046, 15)\n",
            "Split in features and labels\n",
            "Replace nas\n",
            "Impute nas\n",
            "train\n",
            "predict\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "submission.to_csv(\"/content/drive/MyDrive/aml/data/heloc_sub_1.csv\", index=False)"
      ],
      "metadata": {
        "id": "rZcu1IEtrrfb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}