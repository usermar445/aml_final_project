{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "i8v5Nsi8cBmm",
        "cYFYalkGsv2R"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUmyuCHomafl",
        "outputId": "94654145-e3cf-4f77-8202-1bad792f3d82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# If notebook is run on google colab\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "from sklearn.datasets import make_circles, make_classification, make_moons\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "from sklearn.inspection import DecisionBoundaryDisplay\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "import lightgbm as lgb\n",
        "\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.decomposition import PCA\n"
      ],
      "metadata": {
        "id": "j5TkfLKpmeEa"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "Dl_5leCYmluK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Import Higgs"
      ],
      "metadata": {
        "id": "TSY9mKP3mwyP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_higgs = pd.read_csv(\"/content/drive/MyDrive/00 UvA MASTER DATA SCIENCE /05. Applied Machine Learning /higgs_train.csv\")\n",
        "higgs_test = pd.read_csv(\"/content/drive/MyDrive/00 UvA MASTER DATA SCIENCE /05. Applied Machine Learning /benchmark-tabular-ml/higgs_test.csv\")  # adjust path as necessary\n",
        "higgs_test_submission = pd.read_csv(\"/content/drive/MyDrive/00 UvA MASTER DATA SCIENCE /05. Applied Machine Learning /benchmark-tabular-ml/higgs_test_submission.csv\")\n",
        "\n",
        "# drop EventId column -> just some Id we dont need\n",
        "df_higgs = df_higgs.drop(['EventId'], axis=1)\n",
        "higgs_test_data = higgs_test.drop(['EventId'], axis=1)\n",
        "\n",
        "# change target variable\n",
        "df_higgs['Label'] = df_higgs['Label'].replace({'b': 0, 's': 1})\n",
        "\n",
        "# target variable\n",
        "higgs_target_variable = 'Label'"
      ],
      "metadata": {
        "id": "zymhryJummJD"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Import Heloc"
      ],
      "metadata": {
        "id": "zle1NsFRm2jA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/00 UvA MASTER DATA SCIENCE /05. Applied Machine Learning /benchmark-tabular-ml/\"\n",
        "\n",
        "df_heloc = pd.read_csv(path+\"heloc_train.csv\")\n",
        "synthetic_data = pd.read_csv(path+'synthetic_data_heloc_2.csv')\n",
        "heloc_test = pd.read_csv(path+\"heloc_test.csv\")  # adjust path as necessary\n",
        "heloc_test_submission = pd.read_csv(path+\"heloc_test_submission.csv\")\n",
        "\n",
        "# change target variable\n",
        "df_heloc['RiskPerformance'] = df_heloc['RiskPerformance'].replace(('Bad', 'Good'), (0, 1))\n",
        "\n",
        "# target variale\n",
        "heloc_target_variable = 'RiskPerformance'\n"
      ],
      "metadata": {
        "id": "Xrl2l_OIm1Z1"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Heloc Synthetic EDA"
      ],
      "metadata": {
        "id": "mM0Ivq5pIsCe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import shuffle  # For shuffling the dataset\n",
        "\n",
        "# Concatenate the datasets\n",
        "combined_data = pd.concat([df_heloc, synthetic_data], ignore_index=True)\n",
        "\n",
        "# Optional: Shuffle the combined dataset\n",
        "combined_data = shuffle(combined_data, random_state=42)  # '42' is just an example seed for reproducibility\n",
        "\n",
        "# Reset index after shuffling (optional but recommended)\n",
        "combined_data.reset_index(drop=True, inplace=True)\n",
        "\n",
        "df_heloc['RiskPerformance'] = df_heloc['RiskPerformance'].replace(('Bad', 'Good'), (0, 1))\n",
        "combined_data['RiskPerformance'] = combined_data['RiskPerformance'].replace(('Bad', 'Good'), (0, 1))\n",
        "synthetic_data['RiskPerformance'] = df_heloc['RiskPerformance'].replace(('Bad', 'Good'), (0, 1))\n"
      ],
      "metadata": {
        "id": "iRGiIM7c5p-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Import CoverType"
      ],
      "metadata": {
        "id": "hxLYQPVurKIp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/00 UvA MASTER DATA SCIENCE /05. Applied Machine Learning /benchmark-tabular-ml/\"\n",
        "\n",
        "df_cov = pd.read_csv(path+\"covtype_train.csv\")\n",
        "cov_test = pd.read_csv(path+\"covtype_test.csv\")  # adjust path as necessary\n",
        "cov_test_submission = pd.read_csv(path+\"covtype_test_submission.csv\")\n",
        "cov_target_variable = 'Cover_Type'"
      ],
      "metadata": {
        "id": "nAXqURZgrGI5"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### NN + Smote + Scaling (CoverType)"
      ],
      "metadata": {
        "id": "4RLjbwGKuXFw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 256)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.batchnorm2 = nn.BatchNorm1d(128)\n",
        "        self.fc3 = nn.Linear(128, 64)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc4 = nn.Linear(64, 7)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.batchnorm2(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.relu3(x)\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "\n",
        "def train_and_predict_multiclass_neural_network(data, target_col, test_data, test_submission, document_name,num_epochs=10, batch_size=64, learning_rate=0.001):\n",
        "    # Prepare training data\n",
        "    X_train = data.drop(target_col, axis=1).values\n",
        "    y_train = data[target_col].values - 1  # Zero-indexing the target\n",
        "    X_test = test_data.values\n",
        "\n",
        "    # Scale the data\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # Apply SMOTE\n",
        "    smote = SMOTE()\n",
        "    X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)\n",
        "\n",
        "    # Convert to PyTorch tensors\n",
        "    X_train_tensor = torch.tensor(X_train_smote, dtype=torch.float32)\n",
        "    y_train_tensor = torch.tensor(y_train_smote, dtype=torch.long)\n",
        "    X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
        "\n",
        "    # Initialize the model\n",
        "    input_size = X_train_smote.shape[1]\n",
        "    model = NeuralNet(input_size)\n",
        "\n",
        "    # Loss and Optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # DataLoader\n",
        "    train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Training Loop\n",
        "    for epoch in range(num_epochs):\n",
        "        for inputs, labels in train_loader:\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "        # Predictions\n",
        "    model.eval()\n",
        "    test_predictions = []\n",
        "    with torch.no_grad():\n",
        "        for inputs in DataLoader(X_test_tensor, batch_size=batch_size):\n",
        "            outputs = model(inputs)\n",
        "            predicted = torch.argmax(outputs, axis=1)\n",
        "            test_predictions.extend(predicted.numpy())\n",
        "\n",
        "    # Correcting for zero-indexing\n",
        "    test_predictions_corrected = np.array(test_predictions) + 1\n",
        "\n",
        "        # Add corrected predictions to test_submission DataFrame\n",
        "    test_submission['Prediction'] = test_predictions_corrected\n",
        "\n",
        "    # Save the updated DataFrame to CSV\n",
        "    test_submission.to_csv(document_name+'.csv', index=False)\n",
        "\n",
        "    return test_submission\n"
      ],
      "metadata": {
        "id": "1wDQbokgvhw_"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document_name = \"covtype_NN(sm+sc)\"\n",
        "result_submission = train_and_predict_multiclass_neural_network(df_cov, cov_target_variable, cov_test, cov_test_submission, document_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFWa7hlQvmqA",
        "outputId": "fd4dd99d-c542-435a-aa70-b9d7b76a4b52"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.8035\n",
            "Epoch [2/10], Loss: 0.5344\n",
            "Epoch [3/10], Loss: 0.2489\n",
            "Epoch [4/10], Loss: 0.2331\n",
            "Epoch [5/10], Loss: 0.4406\n",
            "Epoch [6/10], Loss: 0.3875\n",
            "Epoch [7/10], Loss: 0.2461\n",
            "Epoch [8/10], Loss: 0.3960\n",
            "Epoch [9/10], Loss: 0.5543\n",
            "Epoch [10/10], Loss: 0.3304\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### NN No Smote, No Scaling (CovType)"
      ],
      "metadata": {
        "id": "rNC9oM_o30Zx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 256)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.batchnorm2 = nn.BatchNorm1d(128)\n",
        "        self.fc3 = nn.Linear(128, 64)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc4 = nn.Linear(64, 7)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.batchnorm2(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.relu3(x)\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "\n",
        "def train_and_predict_multiclass_neural_network_no_smsc(data, target_col, test_data, test_submission, document_name,num_epochs=10, batch_size=64, learning_rate=0.001):\n",
        "    # Prepare training data\n",
        "    X_train = data.drop(target_col, axis=1).values\n",
        "    y_train = data[target_col].values - 1  # Zero-indexing the target\n",
        "    X_test = test_data.values\n",
        "\n",
        "    # Convert to PyTorch tensors\n",
        "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "    y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "\n",
        "    # Initialize the model\n",
        "    input_size = X_train.shape[1]\n",
        "    model = NeuralNet(input_size)\n",
        "\n",
        "    # Loss and Optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # DataLoader\n",
        "    train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Training Loop\n",
        "    for epoch in range(num_epochs):\n",
        "        for inputs, labels in train_loader:\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "        # Predictions\n",
        "    model.eval()\n",
        "    test_predictions = []\n",
        "    with torch.no_grad():\n",
        "        for inputs in DataLoader(X_test_tensor, batch_size=batch_size):\n",
        "            outputs = model(inputs)\n",
        "            predicted = torch.argmax(outputs, axis=1)\n",
        "            test_predictions.extend(predicted.numpy())\n",
        "\n",
        "    # Correcting for zero-indexing\n",
        "    test_predictions_corrected = np.array(test_predictions) + 1\n",
        "\n",
        "        # Add corrected predictions to test_submission DataFrame\n",
        "    test_submission['Prediction'] = test_predictions_corrected\n",
        "\n",
        "    # Save the updated DataFrame to CSV\n",
        "    test_submission.to_csv(document_name+'.csv', index=False)\n",
        "\n",
        "    return test_submission"
      ],
      "metadata": {
        "id": "DSGcdHVMyRog"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document_name = \"covtype_NN(unprocessed)\"\n",
        "result_submission = train_and_predict_multiclass_neural_network_no_smsc(df_cov, cov_target_variable, cov_test, cov_test_submission, document_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d494vDoM8XqH",
        "outputId": "51acebc5-7aed-4edd-aefa-59a75666a3c9"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.8674\n",
            "Epoch [2/10], Loss: 0.7277\n",
            "Epoch [3/10], Loss: 0.6986\n",
            "Epoch [4/10], Loss: 0.6831\n",
            "Epoch [5/10], Loss: 0.8821\n",
            "Epoch [6/10], Loss: 0.8115\n",
            "Epoch [7/10], Loss: 0.6387\n",
            "Epoch [8/10], Loss: 0.7415\n",
            "Epoch [9/10], Loss: 0.6509\n",
            "Epoch [10/10], Loss: 0.6115\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### NN + Smote + Scaling (Heloc, Higgs)"
      ],
      "metadata": {
        "id": "i8v5Nsi8cBmm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 128)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        return torch.sigmoid(x)\n",
        "\n",
        "def train_and_predict_neural_network(data, target_col, test_data, test_submission, document_name, num_epochs=10, batch_size=64, learning_rate=0.001):\n",
        "    # Prepare training data\n",
        "    X_train = data.drop(target_col, axis=1).values\n",
        "    y_train = data[target_col].values\n",
        "    X_test = test_data.values\n",
        "\n",
        "    # Scale the data\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # Apply SMOTE\n",
        "    smote = SMOTE()\n",
        "    X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)\n",
        "\n",
        "    # Convert to PyTorch tensors\n",
        "    X_train_tensor = torch.tensor(X_train_smote, dtype=torch.float32)\n",
        "    y_train_tensor = torch.tensor(y_train_smote, dtype=torch.float32)\n",
        "    X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
        "\n",
        "    # Define the model\n",
        "    input_size = X_train_smote.shape[1]\n",
        "    model = NeuralNet(input_size)\n",
        "\n",
        "    # Loss and Optimizer\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # DataLoader\n",
        "    train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Training Loop\n",
        "    for epoch in range(num_epochs):\n",
        "        for i, (inputs, labels) in enumerate(train_loader):\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            labels = labels.unsqueeze(1)  # Adjust labels' shape if necessary\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "    # Predictions\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        test_predictions = model(X_test_tensor)\n",
        "        test_predictions = (test_predictions >= 0.5).float().squeeze()  # Convert to binary predictions\n",
        "\n",
        "    # Add predictions to test_submission DataFrame\n",
        "    test_submission['Prediction'] = test_predictions.numpy()\n",
        "\n",
        "    # Save the updated DataFrame to CSV\n",
        "    test_submission.to_csv(document_name+'.csv', index=False)\n",
        "\n",
        "    return test_submission"
      ],
      "metadata": {
        "id": "pGiF5RR5oGNT"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document_name = \"higgs_NN(sm+sc)\"\n",
        "\n",
        "result_submission_higgs = train_and_predict_neural_network(df_higgs, higgs_target_variable, higgs_test_data, higgs_test_submission, document_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_7zNoWioK5j",
        "outputId": "c7cd6007-257a-4842-e568-575621a5d280"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.1577\n",
            "Epoch [2/10], Loss: 0.0921\n",
            "Epoch [3/10], Loss: 0.0162\n",
            "Epoch [4/10], Loss: 0.0048\n",
            "Epoch [5/10], Loss: 0.0061\n",
            "Epoch [6/10], Loss: 0.0004\n",
            "Epoch [7/10], Loss: 0.0664\n",
            "Epoch [8/10], Loss: 0.0007\n",
            "Epoch [9/10], Loss: 0.0041\n",
            "Epoch [10/10], Loss: 0.0199\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "document_name = \"heloc_NN(sm+sc)\"\n",
        "\n",
        "result_submission_heloc = train_and_predict_neural_network(df_heloc, heloc_target_variable, heloc_test, heloc_test_submission, document_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cz-prhAgrc8P",
        "outputId": "871f3b29-ba63-46d2-cc35-1785db4b09a0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.5635\n",
            "Epoch [2/10], Loss: 0.5938\n",
            "Epoch [3/10], Loss: 0.6430\n",
            "Epoch [4/10], Loss: 0.6089\n",
            "Epoch [5/10], Loss: 0.4911\n",
            "Epoch [6/10], Loss: 0.5258\n",
            "Epoch [7/10], Loss: 0.3464\n",
            "Epoch [8/10], Loss: 0.5952\n",
            "Epoch [9/10], Loss: 0.5300\n",
            "Epoch [10/10], Loss: 0.5595\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Neural Net No Smote + No Scaling (Heloc & Higgs)"
      ],
      "metadata": {
        "id": "cYFYalkGsv2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 128)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        return torch.sigmoid(x)\n",
        "\n",
        "def train_and_predict_neural_network_no_smsc(data, target_col, test_data, test_submission, document_name, num_epochs=10, batch_size=64, learning_rate=0.001):\n",
        "    # Prepare training data\n",
        "    X_train = data.drop(target_col, axis=1).values\n",
        "    y_train = data[target_col].values\n",
        "    X_test = test_data.values\n",
        "\n",
        "    # Convert to PyTorch tensors\n",
        "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
        "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "\n",
        "    # Define the model\n",
        "    input_size = X_train.shape[1]\n",
        "    model = NeuralNet(input_size)\n",
        "\n",
        "    # Loss and Optimizer\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # DataLoader\n",
        "    train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Training Loop\n",
        "    for epoch in range(num_epochs):\n",
        "        for i, (inputs, labels) in enumerate(train_loader):\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            labels = labels.unsqueeze(1)  # Adjust labels' shape if necessary\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "    # Predictions\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        test_predictions = model(X_test_tensor)\n",
        "        test_predictions = (test_predictions >= 0.5).float().squeeze()  # Convert to binary predictions\n",
        "\n",
        "    # Add predictions to test_submission DataFrame\n",
        "    test_submission['Prediction'] = test_predictions.numpy()\n",
        "\n",
        "    # Save the updated DataFrame to CSV\n",
        "    test_submission.to_csv(document_name+'.csv', index=False)\n",
        "\n",
        "    return test_submission"
      ],
      "metadata": {
        "id": "P1fxdGros2qf"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document_name = \"heloc_NN(unprocessed)\"\n",
        "\n",
        "result_submission_heloc_unprocessed = train_and_predict_neural_network_no_smsc(df_heloc, heloc_target_variable, heloc_test, heloc_test_submission, document_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQE9MdvOtfnv",
        "outputId": "62650fab-6621-41d3-bc32-b50854d5a868"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.3543\n",
            "Epoch [2/10], Loss: 0.2676\n",
            "Epoch [3/10], Loss: 0.8077\n",
            "Epoch [4/10], Loss: 0.4615\n",
            "Epoch [5/10], Loss: 0.7588\n",
            "Epoch [6/10], Loss: 0.5597\n",
            "Epoch [7/10], Loss: 0.6640\n",
            "Epoch [8/10], Loss: 0.8600\n",
            "Epoch [9/10], Loss: 0.5038\n",
            "Epoch [10/10], Loss: 0.5895\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "document_name = \"higgs_NN(unprocessed)\"\n",
        "\n",
        "result_submission_higgs = train_and_predict_neural_network_no_smsc(df_higgs, higgs_target_variable, higgs_test_data, higgs_test_submission, document_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTmRveV-tsi5",
        "outputId": "36984a6d-2060-4e81-a62c-062ba2262e32"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.2709\n",
            "Epoch [2/10], Loss: 0.1041\n",
            "Epoch [3/10], Loss: 0.0186\n",
            "Epoch [4/10], Loss: 0.3512\n",
            "Epoch [5/10], Loss: 0.1263\n",
            "Epoch [6/10], Loss: 0.0201\n",
            "Epoch [7/10], Loss: 0.1708\n",
            "Epoch [8/10], Loss: 0.0596\n",
            "Epoch [9/10], Loss: 0.0286\n",
            "Epoch [10/10], Loss: 0.2828\n"
          ]
        }
      ]
    }
  ]
}